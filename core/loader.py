import hashlib
import os
from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter, Language

# =============================================================================
# PARENT DOCUMENT RETRIEVAL CONFIG
# =============================================================================
# Small chunks for precise search, parent chunks for context
PARENT_CHUNK_SIZE = 2000   # ƒêo·∫°n vƒÉn cha (g·ª≠i cho AI) - Context r·ªông
CHILD_CHUNK_SIZE = 400     # ƒêo·∫°n vƒÉn con (ƒë·ªÉ search) - T√¨m ki·∫øm ch√≠nh x√°c
PARENT_OVERLAP = 200
CHILD_OVERLAP = 50


def get_splitting_strategy(file_path):
    """
    H√†m x√°c ƒë·ªãnh chi·∫øn thu·∫≠t c·∫Øt d·ª±a tr√™n lo·∫°i file
    Tr·∫£ v·ªÅ: parent_size, child_size, parent_overlap, child_overlap, separators
    """
    ext = os.path.splitext(file_path)[1].lower()
    
    # CHI·∫æN THU·∫¨T 1: T√ÄI LI·ªÜU VƒÇN B·∫¢N (S√°ch, B√°o c√°o, Lu·∫≠n vƒÉn)
    if ext in ['.pdf', '.docx', '.doc']:
        return 2500, 500, 300, 80, ["\n\n", "\n", ". ", " ", ""]
        
    # CHI·∫æN THU·∫¨T 2: M√É NGU·ªíN (Code)
    elif ext in ['.py', '.js', '.java', '.cpp', '.html']:
        return 1500, 400, 100, 30, [
            "\nclass ", "\ndef ", "\nfunction ",
            "\n\n", "\n", " "
        ]
        
    # CHI·∫æN THU·∫¨T 3: D·ªÆ LI·ªÜU C·∫§U TR√öC (JSON, CSV)
    elif ext in ['.json', '.csv', '.xml']:
        return 1000, 300, 50, 0, ["\n", "},", "],", " "]
        
    # CHI·∫æN THU·∫¨T 4: M·∫∂C ƒê·ªäNH
    else:
        return 2000, 400, 200, 50, ["\n\n", "\n", " ", ""]


def load_and_split_document(file_path, use_parent_retrieval=True):
    """
    Load v√† split document v·ªõi h·ªó tr·ª£ Parent Document Retrieval.
    
    Khi use_parent_retrieval=True:
    - T·∫°o small chunks (child) ƒë·ªÉ search ch√≠nh x√°c
    - L∆∞u parent content trong metadata ƒë·ªÉ g·ª≠i cho AI context r·ªông h∆°n
    """
    # 1. Load file
    ext = os.path.splitext(file_path)[1].lower()
    if ext == ".pdf":
        loader = PyPDFLoader(file_path)
    elif ext == ".txt":
        loader = TextLoader(file_path, encoding="utf-8")
    elif ext == ".docx":
        loader = Docx2txtLoader(file_path)
    elif ext == ".py":
        loader = TextLoader(file_path, encoding="utf-8") 
    else:
        loader = TextLoader(file_path, encoding="utf-8")
    
    docs = loader.load()
    filename = os.path.basename(file_path)

    # 2. L·∫•y chi·∫øn thu·∫≠t c·∫Øt
    parent_size, child_size, parent_overlap, child_overlap, separators = get_splitting_strategy(file_path)
    
    print(f"‚öôÔ∏è Auto-Config cho {ext}: Parent={parent_size}, Child={child_size}")

    if not use_parent_retrieval:
        # Fallback: ch·ªâ d√πng single-level chunking
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=parent_size,
            chunk_overlap=parent_overlap,
            separators=separators
        )
        splits = splitter.split_documents(docs)
        
        for i, split in enumerate(splits):
            split.metadata["source"] = filename
            split.metadata["chunk_index"] = i
            split.metadata["parent_content"] = split.page_content  # Parent = b·∫£n th√¢n
            unique_string = f"{filename}_{i}"
            split.id = hashlib.sha256(unique_string.encode()).hexdigest()
        
        return splits

    # 3. PARENT DOCUMENT RETRIEVAL
    # B∆∞·ªõc 3a: T·∫°o Parent Chunks (ƒëo·∫°n vƒÉn l·ªõn)
    parent_splitter = RecursiveCharacterTextSplitter(
        chunk_size=parent_size,
        chunk_overlap=parent_overlap,
        separators=separators
    )
    parent_docs = parent_splitter.split_documents(docs)
    
    # B∆∞·ªõc 3b: T·∫°o Child Chunks t·ª´ m·ªói Parent
    child_splitter = RecursiveCharacterTextSplitter(
        chunk_size=child_size,
        chunk_overlap=child_overlap,
        separators=separators
    )
    
    all_child_chunks = []
    
    for parent_idx, parent_doc in enumerate(parent_docs):
        parent_content = parent_doc.page_content
        
        # T·∫°o child chunks t·ª´ parent n√†y
        # Ph·∫£i t·∫°o document gi·∫£ ƒë·ªÉ split
        from langchain_core.documents import Document
        temp_doc = Document(page_content=parent_content, metadata=parent_doc.metadata.copy())
        child_chunks = child_splitter.split_documents([temp_doc])
        
        # G·∫Øn metadata cho m·ªói child chunk
        for child_idx, child_chunk in enumerate(child_chunks):
            child_chunk.metadata["source"] = filename
            child_chunk.metadata["parent_index"] = parent_idx
            child_chunk.metadata["child_index"] = child_idx
            child_chunk.metadata["chunk_index"] = len(all_child_chunks)
            
            # L∆ØU PARENT CONTENT - ƒê√¢y l√† ƒëi·ªÉm quan tr·ªçng!
            # Khi retrieve, s·∫Ω tr·∫£ v·ªÅ parent_content thay v√¨ child content
            child_chunk.metadata["parent_content"] = parent_content
            child_chunk.metadata["parent_page"] = parent_doc.metadata.get("page", 0)
            
            # T·∫°o unique ID cho child
            unique_string = f"{filename}_p{parent_idx}_c{child_idx}"
            child_chunk.id = hashlib.sha256(unique_string.encode()).hexdigest()
            
            all_child_chunks.append(child_chunk)
    
    print(f"üìÑ ƒê√£ t·∫°o {len(parent_docs)} parent chunks ‚Üí {len(all_child_chunks)} child chunks")
    
    return all_child_chunks


def load_document_simple(file_path):
    """
    Load document kh√¥ng c·∫Øt - D√πng cho t√≥m t·∫Øt ho·∫∑c x·ª≠ l√Ω ƒë·∫∑c bi·ªát
    """
    ext = os.path.splitext(file_path)[1].lower()
    if ext == ".pdf":
        loader = PyPDFLoader(file_path)
    elif ext == ".txt":
        loader = TextLoader(file_path, encoding="utf-8")
    elif ext == ".docx":
        loader = Docx2txtLoader(file_path)
    else:
        loader = TextLoader(file_path, encoding="utf-8")
    
    return loader.load()