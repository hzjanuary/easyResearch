import os
import torch
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, AIMessage
from sentence_transformers import CrossEncoder # Cáº§n cÃ i: pip install sentence-transformers
from core.embedder import embedding_model 

load_dotenv()

# Cáº¥u hÃ¬nh Chroma Path vÃ  Thiáº¿t bá»‹
CHROMA_DIR = "database/chroma_db"
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

# Khá»Ÿi táº¡o Reranker mÃ´ hÃ¬nh MiniLM (Tá»‘i Æ°u cho VRAM 4GB cá»§a RTX 3050)
# MÃ´ hÃ¬nh nÃ y so sÃ¡nh trá»±c tiáº¿p Query vÃ  Context Ä‘á»ƒ cháº¥m Ä‘iá»ƒm Ä‘á»™ liÃªn quan
reranker_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device=DEVICE)

# Prompt ngá»¯ cáº£nh hÃ³a cÃ¢u há»i (Contextualization)
contextualize_q_system_prompt = (
    "Given a chat history and the latest user question "
    "which might reference context in the chat history, "
    "formulate a standalone question which can be understood "
    "without the chat history. Do NOT answer the question, "
    "just reformulate it if needed and otherwise return it as is."
)

contextualize_q_prompt = ChatPromptTemplate.from_messages([
    ("system", contextualize_q_system_prompt),
    ("placeholder", "{chat_history}"),
    ("human", "{input}"),
])

# Prompt Ä‘a ngÃ´n ngá»¯
rag_prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        "You are a helpful AI assistant. "
        "Answer the user's question based ONLY on the provided context below. "
        "If the answer is not in the context, simply say you don't know in the user's language. "
        "\n\nIMPORTANT: Detect the language of the user's question (Vietnamese or English) and answer in that SAME language."
    ),
    (
        "human",
        "Context:\n{context}\n\nQuestion:\n{question}"
    )
])

def query_rag_system(question: str, collection_name: str, chat_history: list = None, k_target: int = 10, user_api_key: str = None):
    """
    HÃ m xá»­ lÃ½ RAG káº¿t há»£p Reranking vÃ  Chat History Contextualization.
    """
    
    # 1. XÃ¡c Ä‘á»‹nh dÃ¹ng Key nÃ o
    system_key = os.getenv("GROQ_API_KEY")
    final_api_key = user_api_key if user_api_key and user_api_key.strip() else system_key
    
    if not final_api_key:
        return {
            "answer": "âŒ Lá»—i: Thiáº¿u API Key Groq.",
            "sources": []
        }

    # 2. Khá»Ÿi táº¡o LLM Dynamic
    try:
        llm = ChatGroq(
            model="llama-3.3-70b-versatile",
            temperature=0.2,
            max_tokens=1024,
            api_key=final_api_key
        )
    except Exception as e:
        return {"answer": f"Lá»—i khá»Ÿi táº¡o LLM: {str(e)}", "sources": []}

    # 3. NGá»® Cáº¢NH HÃ“A CÃ‚U Há»I (Náº¿u cÃ³ lá»‹ch sá»­ chat)
    # Viáº¿t láº¡i cÃ¢u há»i Ä‘á»ƒ AI hiá»ƒu ngá»¯ cáº£nh tá»« cuá»™c trÃ² chuyá»‡n trÆ°á»›c Ä‘Ã³
    standalone_question = question
    if chat_history and len(chat_history) > 1:  # Cáº§n Ã­t nháº¥t 1 cáº·p há»i-Ä‘Ã¡p
        try:
            contextualize_chain = contextualize_q_prompt | llm
            # Chuyá»ƒn Ä‘á»•i list messages tá»« Streamlit sang dáº¡ng LangChain message
            history_langchain = []
            for msg in chat_history[:-1]:  # Bá» tin nháº¯n cuá»‘i (cÃ¢u há»i hiá»‡n táº¡i)
                if msg["role"] == "user":
                    history_langchain.append(HumanMessage(content=msg["content"]))
                else:
                    history_langchain.append(AIMessage(content=msg["content"]))
            
            # Viáº¿t láº¡i cÃ¢u há»i
            standalone_question = contextualize_chain.invoke({
                "chat_history": history_langchain,
                "input": question
            }).content
            print(f"ğŸ” CÃ¢u há»i Ä‘Ã£ Ä‘Æ°á»£c lÃ m rÃµ: {standalone_question}")
        except Exception as e:
            print(f"âš ï¸ KhÃ´ng thá»ƒ ngá»¯ cáº£nh hÃ³a cÃ¢u há»i: {e}")
            standalone_question = question

    # 4. Káº¿t ná»‘i DB
    db = Chroma(
        collection_name=collection_name,
        persist_directory=CHROMA_DIR,
        embedding_function=embedding_model 
    )

    # 5. GIAI ÄOáº N 1: Retrieval (Láº¥y rá»™ng - k_target * 2)
    # ChÃºng ta láº¥y nhiá»u á»©ng viÃªn hÆ¡n Ä‘á»ƒ Reranker cÃ³ dá»¯ liá»‡u lá»c
    # Sá»­ dá»¥ng standalone_question (Ä‘Ã£ Ä‘Æ°á»£c ngá»¯ cáº£nh hÃ³a) Ä‘á»ƒ tÃ¬m kiáº¿m chÃ­nh xÃ¡c hÆ¡n
    retriever = db.as_retriever(
        search_type="similarity", # DÃ¹ng similarity Ä‘á»ƒ láº¥y thÃ´ nhanh nháº¥t
        search_kwargs={"k": k_target * 2} 
    )
    initial_docs = retriever.invoke(standalone_question)

    if not initial_docs:
        return {
            "answer": "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin nÃ y trong tÃ i liá»‡u.",
            "sources": [],
            "raw_docs": []
        }

    # 6. GIAI ÄOáº N 2: Reranking (Lá»c tinh báº±ng Cross-Encoder)
    # Táº¡o cáº·p [CÃ¢u há»i Ä‘Ã£ ngá»¯ cáº£nh hÃ³a, Äoáº¡n vÄƒn] Ä‘á»ƒ Reranker cháº¥m Ä‘iá»ƒm
    pairs = [[standalone_question, doc.page_content] for doc in initial_docs]
    scores = reranker_model.predict(pairs)

    # Gáº¯n Ä‘iá»ƒm sá»‘ vÃ o metadata vÃ  sáº¯p xáº¿p láº¡i
    for i, doc in enumerate(initial_docs):
        doc.metadata["rerank_score"] = float(scores[i])
    
    # Sáº¯p xáº¿p giáº£m dáº§n theo Ä‘iá»ƒm vÃ  láº¥y ra Ä‘Ãºng k_target Ä‘oáº¡n tá»‘t nháº¥t
    reranked_docs = sorted(initial_docs, key=lambda x: x.metadata["rerank_score"], reverse=True)[:k_target]

    # 7. GhÃ©p Context & Tráº£ lá»i
    # LÆ¯U Ã: DÃ¹ng cÃ¢u há»i gá»‘c (question) Ä‘á»ƒ AI tráº£ lá»i tá»± nhiÃªn
    context_text = "\n\n".join(d.page_content for d in reranked_docs)
    messages = rag_prompt.format_messages(context=context_text, question=question)
    
    try:
        response = llm.invoke(messages)
        answer_text = response.content.strip()
    except Exception as e:
        answer_text = f"âŒ Lá»—i khi gá»i Groq API: {str(e)}"

    source_names = list(set([d.metadata.get("source", "Unknown") for d in reranked_docs]))

    # Tráº£ vá» káº¿t quáº£ kÃ¨m theo Score Ä‘á»ƒ báº¡n Debug trÃªn giao diá»‡n
    return {
        "answer": answer_text,
        "sources": source_names,
        "raw_docs": [f"[Re-rank Score: {d.metadata['rerank_score']:.2f}] {d.page_content}" for d in reranked_docs]
    }